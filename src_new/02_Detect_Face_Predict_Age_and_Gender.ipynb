{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use input size 416:\n",
    "- file cfg\n",
    "- yolo3_weight.h5 when convert\n",
    "- file train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import colorsys\n",
    "import cv2\n",
    "import dlib\n",
    "# import face_recognition\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
    "from yolo3.utils import letterbox_image\n",
    "\n",
    "# Super resolution\n",
    "from model import Generator\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import inception_resnet_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import WebcamVideoStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from threading import Thread\n",
    "# import cv2\n",
    "# import imutils\n",
    "# class WebcamVideoStream:\n",
    "#     def __init__(self, src=0):\n",
    "#         self.stream = cv2.VideoCapture(src)\n",
    "#         self.stream.set(3, 800)\n",
    "#         self.stream.set(4, 600)\n",
    "#         (self.grabbed, self.frame) = self.stream.read()\n",
    "        \n",
    "#         self.stopped = False\n",
    "        \n",
    "#     def start(self):\n",
    "#         # Start the thread to read frames from the video stream\n",
    "#         Thread(target=self.update, args=()).start()\n",
    "#         return self\n",
    "    \n",
    "#     def update(self):\n",
    "#         while True:\n",
    "#             if self.stopped:\n",
    "#                 return\n",
    "            \n",
    "#             (self.grabbed, self.frame) = self.stream.read()\n",
    "            \n",
    "#     def read(self):\n",
    "#         return self.frame\n",
    "    \n",
    "#     def stop(self):\n",
    "#         self.stopped = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/neosai/Documents/projects/deep_face_recognition/weights/models_gender_and_age/model.ckpt-14001\n",
      "restore model!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "# sess = tf.InteractiveSession()\n",
    "sess = tf.Session()\n",
    "images_pl = tf.placeholder(tf.float32, shape=[None, 160, 160, 3], name='input_image')\n",
    "images_norm = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), images_pl)\n",
    "train_mode = tf.placeholder(tf.bool)\n",
    "age_logits, gender_logits, _ = inception_resnet_v1.inference(images_norm, keep_probability=0.8,\n",
    "                                                             phase_train=train_mode,\n",
    "                                                             weight_decay=1e-5)\n",
    "\n",
    "gender = tf.argmax(tf.nn.softmax(gender_logits), 1)\n",
    "age_ = tf.cast(tf.constant([i for i in range(0, 101)]), tf.float32)\n",
    "age = tf.reduce_sum(tf.multiply(tf.nn.softmax(age_logits), age_), axis=1)\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "saver = tf.train.Saver()\n",
    "ckpt = tf.train.get_checkpoint_state(\"/home/neosai/Documents/projects/deep_face_recognition/weights/models_gender_and_age/\")\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"restore model!\")\n",
    "else:\n",
    "    pass\n",
    "    print(\"error\")\n",
    "\n",
    "faces = np.empty((1,160, 160, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpu_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '../logs/human_pose_dataset_2388_size_416_batch_size_4/human_pose_dataset_2388_size_416_batch_size_4.h5'\n",
    "# model_path = '../model_data/yolo_weights_default_416.h5'\n",
    "anchors_path = '/home/neosai/Documents/projects/deep_face_recognition/weights/yolo_anchors.txt'\n",
    "# classes_path = '../model_data/coco_classes.txt'\n",
    "# classes_path = '../model_data/human_pose.txt'\n",
    "\n",
    "model_path = '/home/neosai/Documents/projects/deep_face_recognition/weights/ep069-loss46.542-val_loss45.218.h5'\n",
    "classes_path = '/home/neosai/Documents/projects/deep_face_recognition/weights/face.txt'\n",
    "\n",
    "score = 0.3\n",
    "iou = 0.2\n",
    "model_image_size = (416, 416)\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class\n",
    "classes_path = os.path.expanduser(classes_path)\n",
    "with open(classes_path) as f:\n",
    "    class_names = f.readlines()\n",
    "\n",
    "class_names = [c.strip() for c in class_names]\n",
    "\n",
    "# Anchors\n",
    "anchors_path = os.path.expanduser(anchors_path)\n",
    "with open(anchors_path) as f:\n",
    "    anchors = f.readline()\n",
    "anchors = [float(x) for x in anchors.split(',')]\n",
    "anchors = np.array(anchors).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/neosai/Documents/projects/deep_face_recognition/weights/ep069-loss46.542-val_loss45.218.h5 model, anchors, and classes loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = os.path.expanduser(model_path)\n",
    "assert model_path.endswith('.h5'), 'Keras model end with file .h5'\n",
    "\n",
    "num_anchors = len(anchors)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "is_tiny_version = num_anchors==6\n",
    "try:\n",
    "    yolo_model = load_model(model_path, compile=False)\n",
    "except:\n",
    "    if is_tiny_version:\n",
    "        yolo_model = tiny_yolo_body(Input(shape=(None, None, 3)), num_anchors//2, num_classes)\n",
    "    else:\n",
    "        yolo_model = yolo_body(Input(shape=(None, None, 3)), num_anchors//3, num_classes)\n",
    "    \n",
    "    yolo_model.load_weights(model_path)\n",
    "else:\n",
    "    yolo_model.layers[-1].output_shape[-1] == num_anchors/len(yolo_model.output) * (num_classes + 5), 'Mismatch between model and given anchor and class sizes'\n",
    "    \n",
    "print(\"{} model, anchors, and classes loaded.\".format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ix, iy, ex, ey = -1, -1, -1, -1\n",
    "# ix1, iy1, ix2, iy2 = -1, -1, -1, -1\n",
    "# cap_from_stream = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def limit_mem():\n",
    "#     K.get_session().close()\n",
    "#     cfg = K.tf.ConfigProto()\n",
    "#     cfg.gpu_options.allow_growth = True\n",
    "#     K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_rec(event, x, y, flags, param):\n",
    "#     global ix, iy, ex, ey, drawing, mode\n",
    "\n",
    "#     if event == cv2.EVENT_LBUTTONDOWN:\n",
    "#         ix, iy = x, y\n",
    "\n",
    "#     elif event == cv2.EVENT_LBUTTONUP:\n",
    "#         ex, ey = x, y\n",
    "#         cv2.rectangle(param, (ix, iy), (x, y), (0, 255, 0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_line(event, x, y, flags, param):\n",
    "#     global ix1, iy1, ix2, iy2, drawing, mode\n",
    "#     if event == cv2.EVENT_LBUTTONDOWN:\n",
    "#         ix1, iy1 = x, y\n",
    "#         print(ix1, iy1)\n",
    "\n",
    "#     elif event == cv2.EVENT_LBUTTONUP:\n",
    "#         ix2, iy2 = x, y\n",
    "#         print(ix2, iy2)\n",
    "#         cv2.line(param, (ix1, iy1), (x, y), (0, 0, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def super_resolution_image(image, model):\n",
    "#     image = Variable(ToTensor()(image), volatile=True).unsqueeze(0)\n",
    "#     # image = image.cuda()\n",
    "#     out = model(image)\n",
    "#     out_img = ToPILImage()(out[0].data.cpu())\n",
    "#     return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_crop_size(video_path):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if cap_from_stream:\n",
    "#             frame = cv2.resize(frame, (1280, 720))\n",
    "#         cv2.namedWindow('draw_rectangle')\n",
    "#         cv2.setMouseCallback('draw_rectangle', draw_rec, frame)\n",
    "#         # cv2.setMouseCallback(\"draw_rectangle\", draw_line, frame)\n",
    "#         print(\"Choose your area of interest!\")\n",
    "#         while 1:\n",
    "#             cv2.imshow('draw_rectangle', frame)\n",
    "#             k = cv2.waitKey(1) & 0xFF\n",
    "#             if k == ord('a'):\n",
    "#                 cv2.destroyAllWindows()\n",
    "#                 break\n",
    "#         break\n",
    "\n",
    "# def get_crop_size1(video_path):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if cap_from_stream:\n",
    "#             frame = cv2.resize(frame, (1280, 720))\n",
    "#         cv2.namedWindow('draw_line')\n",
    "#         cv2.setMouseCallback('draw_line', draw_line, frame)\n",
    "#         print(\"Choose your area of interest!\")\n",
    "#         while 1:\n",
    "#             cv2.imshow('draw_line', frame)\n",
    "#             k = cv2.waitKey(1) & 0xFF\n",
    "#             if k == ord('a'):\n",
    "#                 cv2.destroyAllWindows()\n",
    "#                 break\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "video_capture = cv2.VideoCapture('/home/neosai/Documents/dataset/camera_supervisor/1_03_H_082018120000.avi')\n",
    "# out = cv2.VideoWriter('/home/neosai/Documents/dataset/camera_supervisor/output.avi',fourcc, 20.0, (1280,960))\n",
    "\n",
    "# out = cv2.VideoWriter('/home/neosai/Documents/dataset/camera_supervisor/1_02_H_082018120000.mp4',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (1280,960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_resolution_image(image, model):\n",
    "    image = Variable(ToTensor()(image), volatile=True).unsqueeze(0)\n",
    "    # image = image.cuda()\n",
    "    out = model(image)\n",
    "    out_img = ToPILImage()(out[0].data.cpu())\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714 822 488 584\n",
      "face 0.66\n",
      "<class 'numpy.ndarray'>\n",
      "714 822 488 584\n",
      "face 0.66\n",
      "<class 'numpy.ndarray'>\n",
      "1034 1095 256 328\n",
      "face 0.37\n",
      "716 821 487 583\n",
      "face 0.65\n",
      "<class 'numpy.ndarray'>\n",
      "717 820 489 582\n",
      "face 0.71\n",
      "<class 'numpy.ndarray'>\n",
      "723 821 486 583\n",
      "face 0.76\n",
      "<class 'numpy.ndarray'>\n",
      "722 824 486 583\n",
      "face 0.67\n",
      "<class 'numpy.ndarray'>\n",
      "1036 1096 257 330\n",
      "face 0.46\n",
      "732 828 481 587\n",
      "face 0.60\n",
      "<class 'numpy.ndarray'>\n",
      "1037 1095 256 333\n",
      "face 0.41\n",
      "734 830 481 583\n",
      "face 0.81\n",
      "<class 'numpy.ndarray'>\n",
      "1036 1096 256 331\n",
      "face 0.42\n",
      "735 831 480 585\n",
      "face 0.77\n",
      "<class 'numpy.ndarray'>\n",
      "920 976 379 445\n",
      "face 0.32\n",
      "736 832 479 585\n",
      "face 0.76\n",
      "<class 'numpy.ndarray'>\n",
      "1037 1097 255 335\n",
      "face 0.31\n",
      "738 830 479 584\n",
      "face 0.80\n",
      "<class 'numpy.ndarray'>\n",
      "920 976 379 446\n",
      "face 0.38\n",
      "1038 1096 256 334\n",
      "face 0.53\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0f58d850ed3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mimage_crop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mimage_crop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mimage_crop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper_resolution_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mimage_crop_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mface_male_resize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_crop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mANTIALIAS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f117d01f0110>\u001b[0m in \u001b[0;36msuper_resolution_image\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# image = image.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mout_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/deep_face_recognition/src_new/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mblock6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mblock7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mblock8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblock7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_image_shape = K.placeholder(shape=(2, ))\n",
    "boxes, scores, classes = yolo_eval(yolo_model.output, anchors, len(class_names), input_image_shape, score_threshold=score, iou_threshold=iou)\n",
    "num_frame = 0\n",
    "font = cv2.FONT_HERSHEY_DUPLEX\n",
    "id_number = 0\n",
    "upscale_factor = 4\n",
    "model_name = \"/home/neosai/Documents/projects/deep_face_recognition/weights/netG_epoch_4_100.pth\"\n",
    "# limit_mem()\n",
    "model = Generator(upscale_factor).eval()\n",
    "# model.cuda()\n",
    "model.load_state_dict(torch.load(model_name, map_location=lambda storage, loc: storage))\n",
    "# Video capture\n",
    "# video_capture = WebcamVideoStream(src=0).start()\n",
    "while (video_capture.isOpened()):\n",
    "\n",
    "# while True:\n",
    "    num_frame += 1\n",
    "\n",
    "    # Read video frame and flip camera\n",
    "    ret, frame = video_capture.read()\n",
    "    print(type(frame))\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "    frame_process = np.copy(frame)\n",
    "#     frame_process = frame_process[]\n",
    "    \n",
    "    frame_process = frame_process[200: 960, 375: 1280]\n",
    "\n",
    "    # Detect state standing and sleeping and sitting\n",
    "    image = Image.fromarray(frame_process)\n",
    "\n",
    "    # Process detect hand and recognition furniture\n",
    "    boxed_image = letterbox_image(image, tuple(reversed(model_image_size)))\n",
    "    image_data = np.array(boxed_image, dtype='float32')\n",
    "    \n",
    "    image_data /= 255.\n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "    \n",
    "    out_boxes, out_scores, out_classes = sess.run([boxes, scores, classes],\n",
    "                                                 feed_dict={\n",
    "                                                     yolo_model.input: image_data,\n",
    "                                                     input_image_shape: [image.size[1], image.size[0]],\n",
    "                                                     K.learning_phase(): 0\n",
    "                                                 })\n",
    "\n",
    "    for i, c in reversed(list(enumerate(out_classes))):\n",
    "        predicted_class = class_names[c]\n",
    "        box = out_boxes[i]\n",
    "        score = out_scores[i]\n",
    "        \n",
    "        label = '{} {:.2f}'.format(predicted_class, score)\n",
    "        \n",
    "        top, left, bottom, right = box  \n",
    "        top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "        left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
    "        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
    "        \n",
    "        left = int(left - (right - left) / 4)\n",
    "        top = int(top - (bottom - top) / 4)\n",
    "        right = int(right + (right - left) / 4)\n",
    "        bottom = int(bottom + (bottom - top) / 4)\n",
    "        \n",
    "        left += 375\n",
    "        right += 375\n",
    "        top += 200\n",
    "        bottom += 200\n",
    "        \n",
    "        if score > 0.50:\n",
    "            image_crop = frame[top:bottom, left:right]\n",
    "            image_crop = Image.fromarray(image_crop, 'RGB')\n",
    "            image_crop = super_resolution_image(image_crop, model)\n",
    "            image_crop_array = np.asarray(image_crop)\n",
    "            face_male_resize = image_crop.resize((160, 160), Image.ANTIALIAS)\n",
    "            face = np.array(face_male_resize)\n",
    "            faces[0, :, :, :] = face\n",
    "            age_predict, gender_predict = sess.run([age, gender], feed_dict={images_pl: faces, train_mode: False})\n",
    "            label_age_gender = \"{}, {}\".format(int(age_predict), \"Female\" if gender_predict == 0 else \"Male\")\n",
    "            id_number += 1\n",
    "            name = \"../image/id_{}, {}\".format(id_number, label_age_gender)\n",
    "            cv2.imwrite(name + \".jpg\", image_crop_array)\n",
    "            cv2.putText(frame, label_age_gender, (left + 6, bottom + 6), font, 1.0, (255, 0, 255), 1)\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 3)\n",
    "        cv2.putText(frame, label, (left + 6, top - 6), font, 1.0, (255, 0, 255), 1)\n",
    "        \n",
    "        print(left, right, top, bottom)\n",
    "        print(label)\n",
    "        \n",
    "    cv2.rectangle(frame, (375, 200), (1280, 960), (0, 0, 255), 3)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "#     out.write(frame)\n",
    "    #\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"data/image/hanoi.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_crop = img[0:800, 1000:1400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not a:\n",
    "    print(\"true\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
